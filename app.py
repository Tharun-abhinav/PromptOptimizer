import streamlit as st
from tokenizer import count_tokens
from llm_costs import estimate_cost, get_all_models
import pandas as pd
import matplotlib.pyplot as plt

st.set_page_config(page_title="LLM Token Cost Analyzer", page_icon="üßÆ")

st.title("üßÆ LLM Token Cost Analyzer")
st.write("Paste your prompt below to analyze token usage and estimated cost across models.")

prompt = st.text_area("üìù Prompt Input", height=200)

if st.button("Analyze"):
    if prompt.strip() == "":
        st.warning("Please enter a prompt.")
    else:
        results = []
        for model in get_all_models():
            tokens = count_tokens(prompt, model)
            cost = estimate_cost(tokens, model)
            results.append({
                "Model": model,
                "Tokens": tokens,
                "Estimated Cost ($)": cost
            })

        # Show as DataFrame
        df = pd.DataFrame(results)
        st.dataframe(df, use_container_width=True)

        # Bar chart
        fig, ax = plt.subplots()
        ax.bar(df["Model"], df["Estimated Cost ($)"], color="#4c91f0")
        ax.set_ylabel("Estimated Cost ($)")
        ax.set_title("Cost Comparison Across LLMs")
        st.pyplot(fig)

llm_choice = st.selectbox(
    "Select the LLM for Prompt Optimization",
    ["gpt-3.5-turbo", "gpt-4", "claude-3-opus", "claude-3-haiku", "perplexity-mixtral", "julias"]
)

from optimizer import rule_based_optimizer

# Optimization Section
st.header("üõ†Ô∏è Prompt Optimizer")

optimized_prompt = rule_based_optimizer(prompt)
tokens_original = count_tokens(prompt, model="gpt-4")
tokens_optimized = count_tokens(optimized_prompt, model="gpt-4")

st.subheader("üìâ Token Usage Comparison")
st.markdown(f"""
- üìù **Original Prompt Tokens:** {tokens_original}  
- ‚ú® **Optimized Prompt Tokens:** {tokens_optimized}  
- üí° **Reduction:** {tokens_original - tokens_optimized} tokens (~{100 * (1 - tokens_optimized / tokens_original):.2f}%)
""")

st.subheader("üîÅ Optimized Prompt")
st.code(optimized_prompt, language="markdown")
st.download_button("‚¨áÔ∏è Download Optimized Prompt", optimized_prompt, file_name="optimized_prompt.txt")

from llm_optimizer import rewrite_prompt_with_llm

st.subheader("ü§ñ GPT-3.5 Optimized Prompt")
if st.button("Generate with GPT-3.5"):
    with st.spinner("Optimizing with LLM..."):
        llm_prompt = rewrite_prompt_with_llm(prompt)
        llm_tokens = count_tokens(llm_prompt, model="gpt-4")
        token_diff = tokens_original - llm_tokens
        st.success("‚úÖ Optimization Complete!")

        st.markdown(f"""
        - üß† **LLM Optimized Tokens:** {llm_tokens}  
        - üßÆ **Saved Tokens:** {token_diff}  
        - ü™ô **Estimated Cost Reduction (GPT-4):** ${estimate_cost(token_diff, 'gpt-4')}
        """)
        st.code(llm_prompt, language="markdown")
        st.download_button("‚¨áÔ∏è Download GPT-Optimized Prompt", llm_prompt, file_name="gpt_optimized_prompt.txt")
